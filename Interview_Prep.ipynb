{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Interview Prep.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXnontZQ42Ch",
        "colab_type": "text"
      },
      "source": [
        "# Missing Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SShUZXZ5T3DH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Deletion methods:\n",
        "1. List wise deletion: Will end up with a smaller set, but will not be artificially inducing bias (in case of imputation)\n",
        "2. Column deletion\n",
        "Imputation methods:\n",
        "(Based on the assumption that missing data is MCAR or at least MAR)\n",
        "1. Numeric variables:\n",
        "> * Mean imputation: Rarely used, because it does not preserve the relationship between variables, reduces standard errors (std dev) leading to type 1 errors, also is affected in case of outliers\n",
        "> * Linear regression imputation: better than mean imputation but still reduces standard errors and introduces bias\n",
        "> * MLE imputation: estimates distribution parameters from the complete dataset and predict the missing values. Difficult to configure\n",
        "> * Multiple imputation: Mean/mode/regressor/classifier imputation done multiple times in parallel and the final results are pooled. \n",
        "> * MICE (Multiple imputation with chained equations): Linear/logistic regression/other classifiers with complete data as predictors and missing values as target. Mean/median/random is used as initial substitution. Regression is repeated multiple times in a sequence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9DcB4MGM1IL",
        "colab_type": "text"
      },
      "source": [
        "#  **Categorical variables encoding techniques**\n",
        "# 1. Label encoding: \n",
        "* not suitable for nominal variables, only ordinal variables\n",
        "\n",
        "# 2. One-hot encoding:\n",
        "* suitable for lesser number of categorical variables (with less categories), because increases dimensionality\n",
        "* not suitable for tree based algorithms: deteriorates model performance and will reduce the chances of the tree picking the categorical variable for the split lesser (less closer to the root)\n",
        "* better suited for linear models\n",
        "* binary encoding is a (log (n+1)/log 2) variation of this\n",
        "\n",
        "# 3. Target encoding:\n",
        "* encoding categories using the mean of the target variable.\n",
        "* works well with both linear and tree-based models\n",
        "* prone to data leakage. Variations to avoid the same:\n",
        "\n",
        "> * cross fold target encoding\n",
        "> * leave one out target encoding\n",
        "\n",
        "# 4. Problem specific feature engineering: TBD examples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cQlpI2U7MKI",
        "colab_type": "text"
      },
      "source": [
        "# **Linear Regression**\n",
        "\n",
        "* Ways to solve linear regression: least squares estimation can either be done by differentiation or using matrices (normal equations(analytical)) gradient descent (numerical solutions). Numerical solutions are used when \n",
        "http://math.mit.edu/~gs/linearalgebra/ila0403.pdf\n",
        "https://machinelearningmastery.com/linear-regression-with-maximum-likelihood-estimation/\n",
        "* univariate regression https://are.berkeley.edu/courses/EEP118/current/derive_ols.pdf\n",
        "\n",
        "\n",
        "(https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/)\n",
        "Assumptions that the data has to satisfy for the model to be valid:\n",
        "* 1) Linearity of predictors: linearity of predicted vs actual plot/residual plot (standardised residual vs predicted) should show no pattern\n",
        "\n",
        "\n",
        "> * use a non-linear model/add non-linear terms\n",
        "\n",
        "\n",
        "* 2) No multicollinearity: checked using VIF (variance inflation factor) https://blog.minitab.com/blog/starting-out-with-statistical-software/what-in-the-world-is-a-vif\n",
        "> * remove correlated variables\n",
        "> * perform dimensionality reduction\n",
        "> * combine correlated variables\n",
        "* 3) Normality of residuals: Anderson-Darling test (p value < 0.05 = non-normal)\n",
        "* 4) No autocorrelation of residuals: Durbin-Watson test\n",
        "> use lag terms\n",
        "* 5) Homoscedasticity: equal variance among error terms.\n",
        "> * use weighted least squares regression\n",
        "> * include missing variables\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12IPwft7oviV",
        "colab_type": "text"
      },
      "source": [
        "**Goodness of fit measures for linear regression:**\n",
        "* R2: how much variation of the total variation does the model explain?\n",
        "* adjusted R2: how much variation explained by of the total variation \n",
        "\n",
        "**Confidence intervals vs Prediction intervals**\n",
        "* CI: represents the probability of finding the *mean* of y (95% probability that confidence intervals contain the mean)\n",
        "* PI: used while prediction of an unknown y (the exact value and not mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuXkm30i0n4w",
        "colab_type": "text"
      },
      "source": [
        "# **Regularised Regression**\n",
        "> * Ridge regression: l2 regularisation\n",
        "> * Lasso regression: l1 regularisation (used for feature selection)\n",
        "when to use what: experimental, but l1 if focus is on feature selection\n",
        "why does l1 norm result in feature selection:\n",
        "https://towardsdatascience.com/regularization-in-machine-learning-connecting-the-dots-c6e030bfaddd\n",
        "\n",
        "\n",
        "> * regularisation requires variables to be standardised because regularisation term is proportional to the coefficient\n",
        "https://www.kaggle.com/questions-and-answers/59305\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UYe8W3nIXEN",
        "colab_type": "text"
      },
      "source": [
        "# **Logistic Regression**\n",
        "\n",
        "> * loss function is logloss/log likelihood (binary)/cross entropy (mutliclass) (penalises larger deviations more compared to smaller deviations)\n",
        "> * \n",
        "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#introduction\n",
        "http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function\n",
        ">* the output probabilty distribution is a bernoulli distribution (for binary), multinomial\n",
        "\n",
        "##### ((loss function is differentiated by predicted y (the function that learns the predictions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv_2cGZvvJQH",
        "colab_type": "text"
      },
      "source": [
        "# **Evaluation Metrics**\n",
        "\n",
        "> * Confusion matrix derived metrics: Accuracy, precision, recall, f1 score (harmonic mean of pr)\n",
        "precision = the number of correctly identified positives among the total identified as positive\n",
        "recall = number of correctly identified positives among the total actual positives\n",
        "> * probability based metrics: logloss \n",
        "> * auroc and aupr curves both have a disadvantage of just being ranking metrics. But auroc has the additional advantage of being bad with imbalanced data (positive class is smaller) which p-r curves are resistant against"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw8bWYe2NJCp",
        "colab_type": "text"
      },
      "source": [
        "## **TBD: Imbalanced data handling, K-fold cross validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOJV_ItIM4Rs",
        "colab_type": "text"
      },
      "source": [
        "# **Random Forest**\n",
        "> * weak decision trees (low bias, high variance) are built in parallel using bagging (bootstrap aggregation: sampling with replacement)\n",
        "> * each tree is built:\n",
        "> > * 2/3rd of the samples are used for building the tree, 1/3rd is used to calculate oob error\n",
        "> > * each tree uses all features, but chooses a random sample of the total set to split a node\n",
        ">>> * gini impurity/information gain: gini impurity is computationally less expensive because it doesn't contain the log term. Regression trees use MSE as the splitting criteria\n",
        ">>* each tree is built without pruning\n",
        "\n",
        "## Favourable characteristics:\n",
        "> * can handle missing data (should experiment with other missing data imputation methods regardless)\n",
        "> * robust to outliers in predictors (not target)\n",
        ">* non-linear classifier\n",
        "\n",
        "## Handling data imbalance for random forest\n",
        ">* weighted random forest: assign higher weights to minority class, so that the classification error for minority class are more heavily penalised\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrCCN4mvxu9O",
        "colab_type": "text"
      },
      "source": [
        "# **XGBoost**\n",
        "\n",
        "* Ensemble model which essentially uses a form of gradient descent to iteratively find an update (a weak learn in this case) in the right direction \n",
        "* Each weak learner is a tree which predicts residuals based on the last predicted target values. \n",
        "* The model building starts with an initial target value which is the same for all instances. The residuals are calculated using this value. The residuals are then scaled using a learning rate. New predictions are calculated using this tree and from them the new pseudo residuals and so on until the residuals are very small or max number of iterations. \n",
        "* The split at each node is decided based on gain and impurity (called similarity) scores \n",
        "* The tree is pruned based on the impurity scores\n",
        "* The output of the leaf node is regularized. The reg parameter shrinks similarity scores which results in more pruning which leads to less overfitting.\n",
        "\n",
        "Advantages:\n",
        "* is fast because it takes the computer hardware into account (parallelisation, cache awareness and hard disk usage for very large datasets)\n",
        "* can handle large feature sets and data\n",
        "* can handle missing values well\n",
        "\n",
        "refs: https://medium.com/syncedreview/tree-boosting-with-xgboost-why-does-xgboost-win-every-machine-learning-competition-ca8034c0b283\n",
        "https://www.youtube.com/watch?v=oRrKeUCEbq8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wStedKt5xufe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEBZZfSYvHhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH-M_Bn57AgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}